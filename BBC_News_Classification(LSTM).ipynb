{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BBC News Classification(LSTM).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX5H0P1dCJyG",
        "outputId": "718b70b7-6089-43cb-cdd2-7e56e849cc96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "STOPWORDS =nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5000\n",
        "embedding_dim = 64\n",
        "max_length = 200\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = '<OOV>'\n",
        "training_portion = .8"
      ],
      "metadata": {
        "id": "qirwTJjPDO0p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles = []\n",
        "labels = []\n",
        "\n",
        "with open(\"/content/bbc-text.csv\", 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        labels.append(row[0])\n",
        "        article = row[1]\n",
        "        for word in STOPWORDS:\n",
        "            token = ' ' + word + ' '\n",
        "            article = article.replace(token, ' ')\n",
        "            article = article.replace(' ', ' ')\n",
        "        articles.append(article)\n",
        "print(len(labels))\n",
        "print(len(articles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV8Fep0UJP7P",
        "outputId": "eb81b23f-6b4a-4671-b341-330db9f86d39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2225\n",
            "2225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(len(articles) * training_portion)\n",
        "\n",
        "train_articles = articles[0: train_size]\n",
        "train_labels = labels[0: train_size]\n",
        "\n",
        "validation_articles = articles[train_size:]\n",
        "validation_labels = labels[train_size:]\n",
        "\n",
        "print(train_size)\n",
        "print(len(train_articles))\n",
        "print(len(train_labels))\n",
        "print(len(validation_articles))\n",
        "print(len(validation_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QMeZ_rBLE5x",
        "outputId": "31d69c85-44e8-4f67-f318-026d85abcc73"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1780\n",
            "1780\n",
            "1780\n",
            "445\n",
            "445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_articles)\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "tDdm78pmLLaF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict(list(word_index.items())[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0I594VtLjpL",
        "outputId": "702a9f38-8b61-4e2c-8616-7882e4589e59"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<OOV>': 1,\n",
              " 'also': 6,\n",
              " 'mr': 3,\n",
              " 'new': 8,\n",
              " 'one': 10,\n",
              " 'people': 7,\n",
              " 'said': 2,\n",
              " 'us': 9,\n",
              " 'would': 4,\n",
              " 'year': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_articles)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n"
      ],
      "metadata": {
        "id": "Jau_WH4TLk_J"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n",
        "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(len(validation_sequences))\n",
        "print(validation_padded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xveeA_XoL5kC",
        "outputId": "4fa3e24d-91c6-4237-c140-14697aa0c9e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "445\n",
            "(445, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6Q311l6L6uj",
        "outputId": "adb8c3f5-28b3-457a-bc08-7579073896f9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tech', 'sport', 'politics', 'entertainment', 'business'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.fit_on_texts(labels)\n",
        "\n",
        "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
        "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))"
      ],
      "metadata": {
        "id": "s-cQONm_L-Gt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
        "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    # use ReLU in place of tanh function since they are very good alternatives of each other.\n",
        "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
        "    # Add a Dense layer with 6 units and softmax activation.\n",
        "    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXUycNNGMBYr",
        "outputId": "4d1c81ef-9cb2-4004-f9b3-d11a7ffcb019"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 64)          320000    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              66048     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 390       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 394,694\n",
            "Trainable params: 394,694\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "fw5AV8i9MR6g"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq),verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVeh58tHMYj8",
        "outputId": "eab5760a-3ab5-4fee-c961-65d0b706face"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "56/56 - 12s - loss: 1.5286 - accuracy: 0.3219 - val_loss: 1.0365 - val_accuracy: 0.6112 - 12s/epoch - 221ms/step\n",
            "Epoch 2/10\n",
            "56/56 - 5s - loss: 0.7975 - accuracy: 0.6444 - val_loss: 0.6210 - val_accuracy: 0.7281 - 5s/epoch - 86ms/step\n",
            "Epoch 3/10\n",
            "56/56 - 5s - loss: 0.4122 - accuracy: 0.8522 - val_loss: 0.3475 - val_accuracy: 0.8742 - 5s/epoch - 87ms/step\n",
            "Epoch 4/10\n",
            "56/56 - 5s - loss: 0.1355 - accuracy: 0.9607 - val_loss: 0.2721 - val_accuracy: 0.9169 - 5s/epoch - 87ms/step\n",
            "Epoch 5/10\n",
            "56/56 - 5s - loss: 0.0640 - accuracy: 0.9848 - val_loss: 0.2835 - val_accuracy: 0.9056 - 5s/epoch - 86ms/step\n",
            "Epoch 6/10\n",
            "56/56 - 5s - loss: 0.0983 - accuracy: 0.9708 - val_loss: 0.3737 - val_accuracy: 0.8787 - 5s/epoch - 87ms/step\n",
            "Epoch 7/10\n",
            "56/56 - 5s - loss: 0.0477 - accuracy: 0.9899 - val_loss: 0.2412 - val_accuracy: 0.9281 - 5s/epoch - 87ms/step\n",
            "Epoch 8/10\n",
            "56/56 - 5s - loss: 0.0202 - accuracy: 0.9966 - val_loss: 0.2176 - val_accuracy: 0.9303 - 5s/epoch - 87ms/step\n",
            "Epoch 9/10\n",
            "56/56 - 5s - loss: 0.0091 - accuracy: 0.9983 - val_loss: 0.2192 - val_accuracy: 0.9371 - 5s/epoch - 86ms/step\n",
            "Epoch 10/10\n",
            "56/56 - 5s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2275 - val_accuracy: 0.9461 - 5s/epoch - 87ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = [\"A WeWork shareholder has taken the company to court over the near-$1.7bn (£1.3bn) leaving package approved for ousted co-founder Adam Neumann.\"]\n",
        "seq = tokenizer.texts_to_sequences(txt)\n",
        "padded = pad_sequences(seq, maxlen=max_length)\n",
        "pred = model.predict(padded)\n",
        "labels = ['sport', 'bussiness', 'politics', 'tech', 'entertainment']\n",
        "print(labels[np.argmax(pred)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1byUYPxxO3Dg",
        "outputId": "74d2e674-6304-4713-a82f-3b371c1f92b2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bussiness\n"
          ]
        }
      ]
    }
  ]
}